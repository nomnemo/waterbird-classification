Modeling Notes – Waterbird Classification
========================================

1. Original data description
----------------------------
1.1 Source imagery
- UAV (drone) imagery of colonial waterbird nesting islands (rookery islands) along the Texas coast.
- High‑resolution parent images (~8192 × 5460 px) captured with a DJI Matrice 300 RTK + Zenmuse P1 camera.
- Each parent image contains many birds in dense colonies, often with occlusion (birds overlapping or hidden by vegetation).

1.2 Annotations and classes
- Human annotators drew bounding boxes around individual birds and assigned species‑level labels.
- The original detection project used:
  - 24 annotation classes based on species and age/flight state (e.g., Brown Pelican Adult, Laughing Gull Adult, Mixed Tern Adult, etc.).
  - For visually ambiguous cases or mixed colonies, some labels were “mixed classes” (e.g., Mixed Terns, Mixed Egrets, Other).
- The distribution is strongly long‑tailed:
  - A few classes (e.g., Mixed Tern Adults, Laughing Gull Adults) dominate.
  - Many species have relatively few labeled examples.

1.3 Tiling and duplicates
- The large parent images were tiled into smaller crops (e.g., 512 × 512) to make processing feasible.
- Birds near tile boundaries or in dense colonies can appear in multiple overlapping tiles.
- Implication:
  - The same physical bird may appear several times across different crops.
  - If we randomly split crops into train/val/test, duplicates of the same bird can leak across splits (data leakage).

2. Class grouping and balancing decisions
-----------------------------------------
2.1 Motivation for grouping rare species
- Training a classifier directly on all original classes is difficult because:
  - Some species have very few examples (extreme class imbalance).
  - The model may overfit or ignore rare species.
- For the first round of modeling, the priority is to:
  - Get a stable classifier on the main, well‑represented classes.
  - Reduce noise and sparsity by merging very rare species into coarser categories.

2.2 Group rare species into an OTHER class
- Script: `scripts/0_group_rare_bird_species.py`.
- Decision:
  - Define a threshold (e.g., < 100 examples) for “rare” species.
  - Merge these rare species into an `OTHER`/mixed category.
- Resulting class list:
  - 15 “focused” classes (e.g., Mixed Tern Adult, Laughing Gull Adult, Brown Pelican Adult, White Ibis Adult, etc.).
  - 1 `OTHER` class that absorbs the remaining 9 rare classes.
  - Final total: 16 classes for the classification task.
- Effect:
  - Reduces the extreme tail of the distribution.
  - Keeps modeling tractable and stabilizes per‑class metrics for the main species of interest.

3. Grouped splits to prevent data leakage
-----------------------------------------
3.1 Why grouped splits are needed
- Because parent images were tiled with overlap, the same physical bird may appear in multiple crops.
- If we split at the crop level (randomly), near‑duplicate crops of the same bird might end up in:
  - Training split, and
  - Validation/test split.
- This would artificially inflate metrics:
  - The model “recognizes” the same bird instance rather than generalizing to new birds or new scenes.

3.2 Group definition: parent image ID
- Each record in `data/metadata_balanced_t100.json` contains:
  - `crop_path`: path to the crop image.
  - `source_image`: path to the original parent image, e.g. `images\\102741 - 00001.jpg`.
- We define a group ID by parsing `source_image`:
  - Extract the numeric prefix before `" - "` (e.g., `102741` from `102741 - 00001.jpg`).
  - Treat this prefix as the “parent image ID”.
- All crops that share the same parent image ID are considered a group and must stay together in the same split.

3.3 Implementing grouped splits (80/10/10)
- Script: `scripts/3_split_data.py`.
- Steps:
  1) Read `data/metadata_balanced_t100.json` into a list of records.
  2) For each record, compute `parent_images_per_record[i] = parent_image_id(source_image)`.
  3) Use scikit‑learn’s `GroupShuffleSplit` to split indices into:
     - Train vs holdout (80% vs 20%) based on groups.
     - Validation vs test (10% vs 10%) by splitting the holdout groups 50/50.
- Key property:
  - No parent image ID appears in more than one split.
  - Therefore, no near‑duplicate crops from the same parent image can leak between train/val/test.
- Outputs:
  - `data/split_train.csv`
  - `data/split_val.csv`
  - `data/split_test.csv`
  Each CSV row corresponds to one crop (with `crop_path`, `species_name`, `source_image`, etc.).

4. Data loader and sampling decisions
-------------------------------------
4.1 Overall goal
- Convert split CSVs into PyTorch `DataLoader`s that:
  - Apply appropriate image transforms (train vs eval).
  - Handle class imbalance (rare vs common species).
  - Expose batches of `(image_tensor, label_tensor)` to the model.

4.2 DataLoader construction (`scripts/4_dataloader.py`)
- Main function: `build_loaders(...)` returns:
  - `dl_train`, `dl_val`, `dl_test`: PyTorch `DataLoader` objects.
  - `meta`: a dict with:
    - `classes`: ordered list of class names.
    - `class2id`: mapping from class name → integer index.
    - `class_counts`: counts per class in the (possibly capped) training set.
    - `class_weights`: inverse‑frequency weights for loss functions.
    - `sizes`: number of samples in each split.

4.3 Per‑class capping (`max_per_class`)
- Motivation:
  - Even after grouping rare species, some classes still dominate (e.g., Mixed Terns, Laughing Gulls).
  - To avoid having a single class make up most of the training data, we cap the number of examples per class.
- Mechanism:
  - `max_per_class` is a hyperparameter (e.g., 50, 100, 200, 500).
  - For each split (train/val/test), we:
    - Shuffle the rows.
    - Group by `species_name`.
    - Keep at most `max_per_class` rows per class (for train).
    - For val/test, use a smaller cap (≈ max_per_class / 5) to keep evaluation reasonably sized.
  - Important: We do NOT upsample.
    - Classes with fewer than `max_per_class` examples keep all their samples.
    - Only very common classes are truncated.
- Effect:
  - Prevents a few species from dominating the dataset.
  - Allows experiments with different “effective dataset sizes” while keeping class balance under control.

4.4 Label mapping
- We build the label map (`class2id`) from the training split only:
  - Get the sorted list of unique `species_name` in the training rows.
  - Assign indices 0, 1, 2, ... in sorted order.
  - Apply the same mapping to validation and test splits.
- This ensures that the model’s output indices are consistent across all splits and experiments.

4.5 Image transforms (`scripts/image_transformer.py`)
- We use Albumentations to define two pipelines:
  1) Training transforms (`build_train_transforms`):
     - Resize/pad so the image is at least `img_size × img_size` while preserving context.
     - Random resized crop to `img_size × img_size`.
     - Horizontal/vertical flips.
     - Small random rotations, shifts, and scales.
     - Color jitter (brightness, contrast, saturation, hue).
     - Coarse dropout (randomly masking small rectangles).
     - Normalize by ImageNet mean and standard deviation.
  2) Evaluation transforms (`build_eval_transforms`):
     - Deterministic resize/pad.
     - Center crop to `img_size × img_size`.
     - ImageNet normalization.
- The helper `get_transforms(img_size, train)` returns the appropriate pipeline.
- Rationale:
  - Training: encourage robustness to variations in pose, lighting, and framing.
  - Evaluation: consistent, deterministic preprocessing for fair comparison across runs.

4.6 BirdDataset (`scripts/bird_dataset.py`)
- `BirdDataset` wraps the rows from `split_*.csv` and, when indexed, returns:
  - `x`: float32 image tensor of shape `(3, H, W)`.
  - `y`: long tensor representing the class index.
- Main steps in `__getitem__`:
  1) Look up the row and convert `species_name` to its integer index via `cls2id`.
  2) Resolve `crop_path` to an absolute filesystem path:
     - If `crop_path` is absolute, use it directly.
     - Otherwise, join it with `IMAGE_ROOT` (e.g., `data/crops`).
  3) Load the image with OpenCV (BGR) and convert to RGB.
  4) If the image is missing/unreadable:
     - Print a warning for the first few cases.
     - Use a black fallback image of size `(missing_size, missing_size, 3)`.
  5) Apply the Albumentations transform (train or eval pipeline).
  6) Convert the result to a PyTorch tensor by transposing to `(C, H, W)`.

4.7 WeightedRandomSampler for class imbalance
- Problem:
  - Even after per‑class caps, class frequencies are still uneven.
  - A standard DataLoader with `shuffle=True` will sample examples uniformly over the dataset, meaning common classes still appear more often.
- Solution: `WeightedRandomSampler` in the training loader.
  - For each training sample, compute a sampling weight:
    - `weight_for_sample = 1.0 / class_counts[species_name]`
  - So:
    - Common classes (many samples) get smaller weights per sample.
    - Rare classes (fewer samples) get larger weights per sample.
  - When the sampler chooses samples according to these weights:
    - Each class contributes roughly equally in expectation.
    - Rare species appear in batches more often than they would under uniform sampling.
- Effect:
  - Training is more balanced across classes.
  - The model receives more gradient updates for rare classes without manually duplicating files.
  - This aligns well with macro‑averaged metrics (macro‑F1, macro mAP), where each class counts equally.


5. Summary of modeling decisions so far
--------------------------------------
1) Start from a detection dataset (cropped birds from UAV imagery) with a long‑tailed species distribution.
2) Group very rare species into an `OTHER`/mixed class to stabilize early experiments and reduce extreme sparsity.
3) Split data by parent image ID (grouped 80/10/10) to prevent data leakage from overlapping tiles of the same bird across train/val/test.
4) Use per‑class caps (`max_per_class`) to control how many examples we use per species and to prevent a few classes from dominating training.
5) Build robust train/val/test DataLoaders that:
   - Apply appropriate augmentations (train vs eval),
   - Map species names to class indices,
   - Handle missing images gracefully,
   - Use `WeightedRandomSampler` to balance batches across classes.
6) Train a Swin‑Tiny classifier on top of this pipeline, with results logged in `runs_swin/` for systematic comparison of hyperparameters and dataset settings.